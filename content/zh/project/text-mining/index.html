---
title: "Text Mining with Silent Spring"
author: "Ruoyu Wang"
# date: '2020-03-08'
categories: []
tags:
  - R
  - Text Mining
summary: "In this report, we will analysis and visualize authors' sediments underneath the plain texts with R."
lastmod: '2020-03-08T15:05:12-08:00'
featured: no
image:
  caption: 'Silent Spring on [PBA Galleries](https://www.pbagalleries.com/view-auctions/catalog/id/57/lot/13930/Silent-Spring)'
  focal_point: ''
  preview_only: no
projects: []
---



<div id="background" class="section level3">
<h3>1. Background</h3>
<p>Silent Spring is a book published in August, 1962 by Rachel Carson. This true story telling book focused the chemical damages on animals and ecosystem. Silent Spring encouraged huge discussion and reflection in the whole society. In this report, we will split the whole book into single words and analyze the sentimental tendency behind the words.</p>
</div>
<div id="tidy-version-of-texts" class="section level3">
<h3>2. Tidy version of texts</h3>
<p>First, read in the book PDF file.</p>
<pre class="r"><code># Store the original pdf file
silent_spring &lt;- &quot;silent_spring.pdf&quot;

# Read in the pdf
sil_spr_text &lt;- pdftools::pdf_text(silent_spring)
# Each page goes into one row</code></pre>
<p>Through <code>sil_spr_text</code>, we can see that the line breakers are <code>\n</code>.</p>
<p>Next, we will split the pages into lines in the cells.</p>
<pre class="r"><code>sil_spr_df &lt;- data.frame(sil_spr_text) %&gt;% 
  dplyr::mutate(text_full = stringr::str_split(sil_spr_text, 
                                               pattern = &quot;\\n&quot;)) %&gt;% 
  # Now each line is in the group of each page
  unnest(text_full) %&gt;% 
  mutate(text_full = stringr::str_trim(text_full)) 
  # remove extra white spaces at the beginning</code></pre>
<p>Now, put each word into its own cell.</p>
<pre class="r"><code>sil_spr_tokens &lt;- sil_spr_df %&gt;% 
  dplyr::select(-sil_spr_text) %&gt;% # only keep the line column
  unnest_tokens(word, text_full) # the new column is &quot;word&quot;</code></pre>
<p>Notice that there are lots of meaningless words and numbers, we can remove the stop words and numeric pieces:</p>
<pre class="r"><code>sil_spr_stop &lt;- sil_spr_tokens %&gt;% 
  anti_join(stop_words) 
  # find and remove all the same words in the stop word list

sil_spr_nonum &lt;- sil_spr_stop %&gt;% 
  dplyr::filter(is.na(as.numeric(word)))
  # filter out all the numbers</code></pre>
</div>
<div id="word-frequency-visualization" class="section level3">
<h3>2. Word frequency visualization</h3>
<div id="a.-the-most-common-meaningful-words-in-the-document" class="section level4">
<h4>a. The most common meaningful words in the document</h4>
<pre class="r"><code># Count the occurences of words
sil_spr_n &lt;- sil_spr_nonum %&gt;% 
  count(word) 

# Make a nice wordcloud
wordcloud(words = sil_spr_n$word, freq = sil_spr_n$n,
          max.words = 200, random.order = FALSE,
          rot.per=0.35, 
          colors=brewer.pal(8, &quot;Dark2&quot;))</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>With the word cloud, we can see the top-200 common words in this book. The font sizes also depend on the words’ frequency.</p>
<pre class="r"><code>sil_spr_top20 &lt;- sil_spr_n %&gt;% 
  arrange(-n) %&gt;% 
  head(20)

# Make a nice lolipop plot for the word occurence
ggplot(data = sil_spr_top20, 
       aes(x = reorder(word, n), y = n)) +
  geom_point(aes(color = word), size = 2,
             show.legend = FALSE) +
  geom_segment(aes(x = word, xend = word,
                   y = 0, yend = n,
                   color = word),
               show.legend = FALSE) +
  labs(x = &quot;&quot;, y = &quot;Occurrence of words&quot;)+
  theme_minimal()+
  coord_flip()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-6-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We can also see the exact occurrence of the top 20 words with a nice lollipop plot!</p>
</div>
</div>
<div id="text-sentimental-analysis" class="section level3">
<h3>3. Text sentimental analysis</h3>
<p>There are three ways to analyze the sentimental in the texts: (i) Binary method, either positive or negative scale; (ii) Ordinal sediments, ranked values from negative to positive (-5 ~ +5); (iii) Descriptive method, words in emotional bins.</p>
</div>
<div id="a.-binary-method" class="section level3">
<h3>a. Binary method</h3>
<pre class="r"><code>ss_bing &lt;- sil_spr_nonum %&gt;% 
  inner_join(get_sentiments(lexicon = &quot;bing&quot;))

# Find counts by sentiments

ss_bing_n &lt;- ss_bing %&gt;% 
  count(sentiment, sort = TRUE) 

ggplot(data = ss_bing_n)+
  geom_col(aes(x = reorder(sentiment, -n), y = n,
               fill = sentiment),
           show.legend = FALSE,
           width = 0.35)+
  labs(x = &quot;Sediments&quot;, y = &quot;Counts&quot;)+
  theme_minimal()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>It seems that the author had more negative emotions than positive ones.</p>
<div id="b.-ordinal-sediments" class="section level4">
<h4>b. Ordinal sediments:</h4>
<p>Polarity sentiment lexicons (ranked values on negative → positive scale) - “afinn”</p>
<p>Polarity scores</p>
<p>Most negative: -5; Most positive: +5.</p>
<pre class="r"><code>sil_spr_afinn &lt;- sil_spr_nonum %&gt;% 
  inner_join(get_sentiments(lexicon = &quot;afinn&quot;))

# count the values of the sentiments
sil_spr_afinn_hist &lt;- sil_spr_afinn %&gt;% 
  count(value)

ggplot(data = sil_spr_afinn_hist, aes(x = value, y = n))+
  geom_col(aes(fill = value), show.legend = FALSE)+
  labs(x = &quot;Sentimental score&quot;, y = &quot;Frequency&quot;)+
  theme_minimal()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-8-1.png" width="672" style="display: block; margin: auto;" /></p>
<pre class="r"><code>sil_spr_summary &lt;- sil_spr_afinn %&gt;% 
  summarize(
    mean_score = mean(value),
    median_score = median(value)
  )</code></pre>
<pre class="r"><code>ss_afinn_n5 &lt;- sil_spr_afinn %&gt;% 
  filter(! value == -5) %&gt;% 
  count(word, value, sort = TRUE) %&gt;% 
  group_by(value) %&gt;% 
  top_n(5) %&gt;%  # col = n
  ungroup()


ggplot(data = ss_afinn_n5,
                      aes(x = reorder(word, -n),
                          y = n,
                          fill = n))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~value, ncol = 2, scales = &quot;free&quot;)+
  labs(x = &quot;&quot;, y = &quot;Frequency&quot;)+
  theme_minimal()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-10-1.png" width="864" style="display: block; margin: auto;" /></p>
<p>Second, let’s test with emotional words method.</p>
<p>The NRC Emotion Lexicon is a list of English words and their associations with eight basic emotions (anger, fear, anticipation, trust, surprise, sadness, joy, and disgust) and two sentiments (negative and positive).</p>
<pre class="r"><code>sil_spr_nrc &lt;- sil_spr_nonum %&gt;% 
  inner_join(get_sentiments(lexicon = &quot;nrc&quot;))

# Find counts by sentiments

ss_nrc_n &lt;- sil_spr_nrc %&gt;% 
  count(sentiment, sort = TRUE) 

ggplot(data = ss_nrc_n)+
  geom_col(aes(x = reorder(sentiment, -n), y = n,
               fill = sentiment),
           show.legend = FALSE)+
  labs(x = &quot;Sediments&quot;, y = &quot;Counts&quot;)+
  theme_minimal()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>How do sentiment/top words compare for speeches/documents/etc. from two or more different groups, candidates, etc.?</p>
<pre class="r"><code>ss_nrc_n5 &lt;- sil_spr_nrc %&gt;% 
  count(word, sentiment, sort = TRUE) %&gt;% 
  group_by(sentiment) %&gt;% 
  top_n(5) %&gt;%  # col = n
  ungroup()


ggplot(data = ss_nrc_n5,
                      aes(x = reorder(word, -n),
                          y = n,
                          fill = sentiment))+
  geom_col(show.legend = FALSE)+
  facet_wrap(~sentiment, ncol = 2, scales = &quot;free&quot;)+
  labs(x = &quot;&quot;, y = &quot;Frequency&quot;)+
  theme_minimal()</code></pre>
<p><img src="/en/project/text-mining/index_files/figure-html/unnamed-chunk-12-1.png" width="768" style="display: block; margin: auto;" /></p>
</div>
</div>
